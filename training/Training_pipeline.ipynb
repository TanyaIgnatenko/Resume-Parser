{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setting Notebook vars\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT_PDF_DIR = Path(\"./data/pdfs\")\n",
        "INPUT_PDF_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUTPUT_JSONL_PATH = Path(\"./data/labelstudio/resumes.jsonl\")\n",
        "OUTPUT_JSONL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# spaCy training paths\n",
        "CONFIG_PATH = Path(\"./config.cfg\")\n",
        "TRAIN_JSON = Path(\"./train.json\")\n",
        "DEV_JSON = Path(\"./dev.json\")\n",
        "TRAIN_SPACY = Pat(\"./train.spacy\")\n",
        "DEV_SPACY = Path(\"./dev.spacy\")\n",
        "OUTPUT_DIR = Path(\"./output\")\n",
        "GPU_ID = 0  # set to -1 to run on CPU"
      ],
      "metadata": {
        "id": "F_95aVKpNaP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Upload data: use json dataset with parsed resumes:"
      ],
      "metadata": {
        "id": "XH71MdiYQ8l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "TRAIN_JSON_PATH = Path(\"./parsed/train.json\")\n",
        "DEV_JSON_PATH = Path(\"./parsed/dev.json\")\n",
        "\n",
        "train_uploader = widgets.FileUpload(accept=\".json\", multiple=False)\n",
        "dev_uploader = widgets.FileUpload(accept=\".json\", multiple=False)\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_save(_):\n",
        "    with out:\n",
        "        out.clear_output()\n",
        "        if not train_uploader.value or not dev_uploader.value:\n",
        "            print(\"Please upload both train.json and dev.json\")\n",
        "            return\n",
        "        t_name, t_meta = next(iter(train_uploader.value.items()))\n",
        "        d_name, d_meta = next(iter(dev_uploader.value.items()))\n",
        "        TRAIN_JSON_PATH.write_bytes(t_meta[\"content\"])\n",
        "        DEV_JSON_PATH.write_bytes(d_meta[\"content\"])\n",
        "        print(f\"Saved train.json → {TRAIN_JSON_PATH}\")\n",
        "        print(f\"Saved dev.json   → {DEV_JSON_PATH}\")\n",
        "\n",
        "btn = widgets.Button(description=\"Save files\", button_style=\"success\")\n",
        "btn.on_click(on_save)\n",
        "\n",
        "display(widgets.HTML(\"<b>Upload train.json</b>\"), train_uploader)\n",
        "display(widgets.HTML(\"<b>Upload dev.json</b>\"), dev_uploader)\n",
        "display(btn, out)"
      ],
      "metadata": {
        "id": "J8SMIDG_Q7No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract text from pdfs (optional)\n"
      ],
      "metadata": {
        "id": "RRLEsNqjy3uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re, sys\n",
        "from pathlib import Path\n",
        "from pdfminer.layout import LAParams\n",
        "\n",
        "USE_PYMUPDF = False\n",
        "try:\n",
        "    import fitz\n",
        "    USE_PYMUPDF = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def extract_text_pdfplumber(pdf_path):\n",
        "    import pdfplumber\n",
        "    text_parts = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text_parts.append(page.extract_text() or \"\")\n",
        "    return \"\\n\\n\".join(text_parts)\n",
        "\n",
        "def extract_text_pymupdf(pdf_path):\n",
        "    text_parts = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text_parts.append(page.get_text(\"text\") or \"\")\n",
        "    return \"\\n\\n\".join(text_parts)\n",
        "\n",
        "def pdf_to_text(pdf_path: Path) -> str:\n",
        "    return extract_text_pdfplumber(str(pdf_path))\n",
        "\n",
        "def convert_pdfs_to_text(in_dir: str, out_jsonl: str):\n",
        "    in_path = Path(in_dir)\n",
        "    pdf_files = sorted(in_path.glob(\"*.pdf\"))\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDFs found in: {in_dir}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    out_path = Path(out_jsonl)\n",
        "    count_ok, count_empty = 0, 0\n",
        "\n",
        "    with out_path.open(\"w\", encoding=\"utf-8\") as fw:\n",
        "        for pdf in pdf_files:\n",
        "            try:\n",
        "                txt = pdf_to_text(pdf)\n",
        "                if not txt or len(txt) < 30:\n",
        "                    count_empty += 1\n",
        "                    print(f\"[WARN] Very little/empty text in {pdf.name} (skipping)\")\n",
        "                    continue\n",
        "\n",
        "                # One LS task per resume\n",
        "                rec = {\"text\": txt, \"meta\": {\"source\": pdf.name}}\n",
        "                fw.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "                count_ok += 1\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] {pdf.name}: {e}\")\n",
        "\n",
        "    print(f\"Done. Wrote {count_ok} items to {out_jsonl}. Skipped empty: {count_empty}\")\n",
        "\n",
        "convert_pdfs_to_text(str(INPUT_PDF_DIR), str(OUTPUT_JSONL_PATH))"
      ],
      "metadata": {
        "id": "GVpSqMYgODuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Normalize and clean the data:\n",
        "\n"
      ],
      "metadata": {
        "id": "4co12NJLzkR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import sys\n",
        "\n",
        "import re\n",
        "\n",
        "CID = re.compile(r\"\\(cid:\\d+\\)\")\n",
        "# replace \"-\\n\" with a space (instead of removing)\n",
        "DEHYPH_LINE = re.compile(r\"-\\s*\\n\\s*\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    # normalize newlines\n",
        "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    # remove CID artifacts\n",
        "    s = CID.sub(\" \", s)\n",
        "\n",
        "    # fix words split by newline without hyphen (e.g., \"Machine\\nLearning\")\n",
        "    s = re.sub(r\"([a-z])\\n([a-z])\", r\"\\1 \\2\", s, flags=re.I)\n",
        "\n",
        "    # collapse multiple spaces/tabs → single space\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "\n",
        "    # collapse 3+ newlines → just 2 newlines\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "\n",
        "    return s.strip()\n",
        "\n",
        "def apply_clean_text(in_path, out_path):\n",
        "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    cleaned = []\n",
        "    for obj in data:\n",
        "        raw = obj.get(\"text\", \"\")\n",
        "        obj[\"text\"] = clean_text(raw)\n",
        "        cleaned.append(obj)\n",
        "\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cleaned, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Cleaned {len(cleaned)} resumes → {out_path}\")\n",
        "\n",
        "apply_clean_text('./parsed/train.json', 'clean/train.json')\n",
        "apply_clean_text('./parsed/dev.json', 'clean/dev.json')\n"
      ],
      "metadata": {
        "id": "uGfpfptMz8gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Make prelabels for resumes to speedup the Annotation process:"
      ],
      "metadata": {
        "id": "2v8MlGUJ0KV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re, sys\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Regex dictionaries -----------------------------------------\n",
        "SKILL_TERMS = [\n",
        "    # Programming languages & core libs\n",
        "    r\"Python\", r\"Java\", r\"JavaScript\", r\"TypeScript\", r\"C\\+\\+\", r\"C#\", r\"\\bC\\b\", r\"Rust\", r\"Go\", r\"R\",\n",
        "    r\"NumPy\", r\"Pandas\", r\"scikit-?learn\", r\"PyTorch\", r\"TensorFlow\", r\"Keras\", r\"LightGBM\",\n",
        "    r\"BeautifulSoup\", r\"Keras\", r\"CatBoost\", r\"Seaborn\", r\"OpenCV\",\n",
        "\n",
        "    # Visualization & analysis\n",
        "    r\"Matplotlib\", r\"Seaborn\", r\"Plotly\", r\"Tableau\", r\"Power BI\", r\"D3\\.js\",\n",
        "    r\"EDA\", r\"Exploratory Data Analysis\", r\"Statistical Analysis\", r\"Statistics\", r\"Statistical Modeling\",\n",
        "    r\"Data Visualization\", r\"Data Cleaning\", r\"Data Preprocessing\", r\"Data Analysis\", r\"Business analytics\",\n",
        "    r\"Regression Analysis\", r\"Classification\", r\"Clustering\", r\"AB-?Testing\", r\"A/B Testing\",\n",
        "\n",
        "    # ML & AI concepts\n",
        "    r\"Supervised learning\", r\"Unsupervised learning\", r\"Machine Learning\",\n",
        "    r\"Deep Learning\", r\"CNNs?\", r\"RNNs?\", r\"Transformers?\",\n",
        "    r\"Reinforcement Learning\", r\"Computer Vision\", r\"Named Entity Recognition\",\n",
        "    r\"Feature Engineering\", r\"Hyperparameter Tuning\", r\"Algorithms\", r\"data structures\",\n",
        "    r\"NLP\", r\"Robotics\", r\"Ray\", r\"MLflow\", r\"JAX\", r\"Hugging Face\", r\"spaCy\",\n",
        "\n",
        "    # Model evaluation metrics\n",
        "    r\"Model Evaluation\", r\"Accuracy\", r\"Precision\", r\"Recall\", r\"F1-?score\", r\"ROC-?AUC\",\n",
        "\n",
        "    # Backend / web frameworks & patterns\n",
        "    r\"Spring Boot\", r\"Spring\", r\"Django\", r\"Django REST Framework\", r\"DRF\", r\"Celery\",\n",
        "    r\"Flask\", r\"REST API\", r\"GraphQL\", r\"Apollo\",\n",
        "    r\"Node\\.js\", r\"Express(\\.js)?\", r\"NestJS\", r\"gRPC\", r\"socket\\.io\",\n",
        "\n",
        "    # Frontend ecosystem\n",
        "    r\"React(\\.js)?\", r\"Next\\.js\", r\"Redux\", r\"React hooks\", r\"React-?router\", r\"Angular\",\n",
        "    r\"redux-?saga\", r\"redux-?thunk\", r\"Effector\", r\"VueJS?\",\n",
        "    r\"HTML5?\", r\"CSS3?\", r\"SCSS\", r\"PostCSS\", r\"JSS\",\n",
        "    r\"CSS Modules\", r\"BEM\", r\"CSS-?in-?JS\", r\"Styled components\",\n",
        "    r\"Material UI\", r\"DOM API\", r\"Canvas API\", r\"SVG\",\n",
        "    r\"PWA\", r\"Web Workers\", r\"Push Notifications\", r\"IndexedDB\",\n",
        "    r\"WebSockets?\", r\"HTTP\", r\"SSR\",\n",
        "    r\"RxJS\", r\"UI/UX design principles\", r\"UX\",\n",
        "\n",
        "    # Mobile Development\n",
        "    r\"Kotlin\", r\"Swift\", r\"SwiftUI\", r\"Firebase\",\n",
        "\n",
        "    # Build & dev tools\n",
        "    r\"Webpack\", r\"Babel\", r\"npm\", r\"yarn\", r\"Bazel\",\n",
        "    r\"ESLint\", r\"Prettier\", r\"Storybook\", r\"Chrome Devtools?\", r\"Figma\", r\"PyCharm\", r\"Jupyter Notebook\",\n",
        "    r\"Maven\", r\"Gradle\", r\"Jenkins\", r\"TeamCity\", r\"Splunk\", r\"Prometheus\", r\"Grafana\",\n",
        "    r\"GitHub Actions\", r\"Selenium\", r\"JMeter\", r\"Postman\",\n",
        "\n",
        "    # DevOps / CI/CD & Infrastructure\n",
        "    r\"CI/CD\", r\"Git\", r\"GitHub\", r\"Bitbucket\", r\"OpenShift\",\n",
        "    r\"Docker\", r\"Kubernetes\", r\"Terraform\", r\"OpenShift\",\n",
        "\n",
        "    # Cloud & big data stack\n",
        "    r\"AWS\", r\"GCP\", r\"Azure\",\n",
        "    r\"EMR\", r\"EC2\", r\"S3\", r\"DynamoDB\", r\"SQS\", r\"SNS\", r\"Lambda\", r\"AWS CDK\",\n",
        "    r\"AWS Step Functions\", r\"AWS Batch\", r\"Athena\",\n",
        "    r\"Elasticsearch\", r\"Elastic ?Search\", r\"Kafka\", r\"Spark\", r\"Hadoop\", r\"Hive\", r\"Presto\", r\"Druid\", r\"Zookeeper\", r\"Qubole\",\n",
        "    r\"Airflow\", r\"BigQuery\",\n",
        "\n",
        "    # Monitoring & Logging\n",
        "    r\"ELK\",\n",
        "\n",
        "    # Security\n",
        "    r\"Kali Linux\", r\"Snort\", r\"Wireshark\",\n",
        "\n",
        "    # Robotics & Embedded Systems\n",
        "    r\"ROS\", r\"Embedded Systems\", r\"Gazebo\",\n",
        "\n",
        "    # Databases\n",
        "    r\"MySQL\", r\"DB2\", r\"MongoDB\", r\"Databases?\", r\"NoSQL\", r\"PostgreSQL\", r\"Oracle\", r\"ClickHouse\", r\"Hazelcast\", r\"\\bSQL\\b\",\n",
        "\n",
        "    # General tools & collaboration\n",
        "    r\"Git\", r\"Linux\", r\"Jupyter Notebook\", r\"APIs?\", r\"Excel\",\n",
        "    r\"Jira\", r\"Confluence\", r\"Cloud platforms?\", r\"Docker\", r\"Bash\", r\"vim\", r\"LATEX\",\n",
        "\n",
        "    # Methodologies & practices\n",
        "    r\"Agile\", r\"Scrum\", r\"SDLC\", r\"Microservices\",\n",
        "    r\"Microservice architecture\", r\"Micro-?frontend architecture\",\n",
        "    r\"Performance Optimization\", r\"Web Security\", r\"SEO\", r\"Web Accessibility\", r\"a11y\",\n",
        "    r\"OOP\", r\"SOLID\", r\"Design patterns\", r\"Clean Code\", r\"REST API\", r\"API development\",\n",
        "    r\"Unit tests?\", r\"Integration tests?\", r\"e2e tests?\", r\"Screenshot tests?\",\n",
        "    r\"Jest\", r\"React-?testing-?library\", r\"Cypress\", r\"Hermione\", r\"RAII\",\n",
        "    r\"Product Roadmaps\", r\"API Design\",\n",
        "\n",
        "    # Soft/role-adjacent technical skills (keep for recall)\n",
        "    r\"Client Requirement Scoping\",\n",
        "    r\"Cross-?functional Collaboration\",\n",
        "    r\"Mentoring\",\n",
        "    r\"Problem solving\",\n",
        "    r\"Debugging\",\n",
        "]\n",
        "\n",
        "LANGUAGE_TERMS = [\n",
        "    r\"(?:German|French|Spanish|Russian|English)(?:(?:\\s+-\\s*|\\s+)(?:native|fluent|advanced|intermediate|beginner|basic|proficient|working\\s+knowledge))?\",\n",
        "    r\"(?:native|fluent|advanced|intermediate|beginner|basic|proficient|working\\s+knowledge)(?:\\s+of)?(?:\\s+(?:German|French|Spanish|Russian|English))\",\n",
        "    r\"German(?:\\s*\\([A-C][12]\\)|\\s*-\\s*(Beginner|Intermediate|Advanced|Fluent)|\\s*B[12]|C[12])?\",\n",
        "    r\"English(?:\\s*\\([A-C][12]\\)|\\s*-\\s*(Beginner|Intermediate|Advanced|Advanced|Fluent))?\",\n",
        "    r\"French(?:\\s*\\([A-C][12]\\)|\\s*-\\s*(Beginner|Intermediate|Advanced|Fluent))?\",\n",
        "    r\"Russian(?:\\s*\\([A-C][12]\\)|\\s*-\\s*(Beginner|Intermediate|Advanced|Fluent))?\",\n",
        "    r\"Spanish(?:\\s*\\([A-C][12]\\)|\\s*-\\s*(Beginner|Intermediate|Advanced|Fluent))?\"\n",
        "]\n",
        "\n",
        "# Cleanup helpers\n",
        "CID = re.compile(r\"\\(cid:\\d+\\)\")\n",
        "DEHYPH = re.compile(r\"-\\s*\\n\\s*\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = CID.sub(\" \", s)\n",
        "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "    s = DEHYPH.sub(\"\", s)                  # join hyphenated line breaks\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def find_spans(text: str, patterns, label):\n",
        "    results = []\n",
        "    for pat in patterns:\n",
        "        for m in re.finditer(rf\"\\b(?:{pat})\\b\", text, flags=re.IGNORECASE):\n",
        "            start, end = m.start(), m.end()\n",
        "            results.append({\n",
        "                \"from_name\": \"label\",\n",
        "                \"to_name\": \"text\",\n",
        "                \"type\": \"labels\",\n",
        "                \"value\": {\n",
        "                    \"start\": start,\n",
        "                    \"end\": end,\n",
        "                    \"text\": text[start:end],\n",
        "                    \"labels\": [label]\n",
        "                }\n",
        "            })\n",
        "    # dedupe overlaps, keep first\n",
        "    results.sort(key=lambda r: (r[\"value\"][\"start\"], r[\"value\"][\"end\"]))\n",
        "    dedup, last_end = [], -1\n",
        "    for r in results:\n",
        "        s, e = r[\"value\"][\"start\"], r[\"value\"][\"end\"]\n",
        "        if s >= last_end:\n",
        "            dedup.append(r)\n",
        "            last_end = e\n",
        "    return dedup\n",
        "\n",
        "def make_prediction_for(text: str):\n",
        "    spans = []\n",
        "    spans += find_spans(text, SKILL_TERMS, \"Skill\")\n",
        "    spans += find_spans(text, LANGUAGE_TERMS, \"Language\")\n",
        "    return {\"result\": spans, \"score\": 0.3, \"model_version\": \"regex_v1\"}\n",
        "\n",
        "def make_prelabels(in_json: str, out_json: str, with_predictions: bool = True):\n",
        "    # Read input as JSON array\n",
        "    data = json.loads(Path(in_json).read_text(encoding=\"utf-8-sig\"))\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(\"Input must be a JSON array of objects\")\n",
        "\n",
        "    tasks = []\n",
        "    for obj in data:\n",
        "        raw = obj.get(\"text\") or obj.get(\"data\", {}).get(\"text\") or \"\"\n",
        "        text = clean_text(raw)\n",
        "        data_field = {\"text\": text}\n",
        "        if \"meta\" in obj:\n",
        "            data_field[\"meta\"] = obj[\"meta\"]\n",
        "\n",
        "        task = {\"data\": data_field}\n",
        "        if with_predictions:\n",
        "            task[\"predictions\"] = [make_prediction_for(text)]\n",
        "        tasks.append(task)\n",
        "\n",
        "    # Write a single valid JSON array\n",
        "    Path(out_json).write_text(json.dumps(tasks, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "    print(f\"Wrote {len(tasks)} tasks to {out_json}\")\n",
        "\n",
        "make_prelabels('.\\clean\\train.json', '.\\prelabeled\\train.json')\n",
        "make_prelabels('.\\clean\\dev.json', '.\\prelabeled\\dev.json')"
      ],
      "metadata": {
        "id": "ll0tMXRg0h_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Annotate the resumes in Label studio"
      ],
      "metadata": {
        "id": "6GZ2i3EF0pqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load annotated resumes:"
      ],
      "metadata": {
        "id": "RTwQ0t3L1DKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "TRAIN_JSON_PATH = Path(\"./annotated/train.json\")\n",
        "DEV_JSON_PATH = Path(\"./annotated/dev.json\")\n",
        "\n",
        "train_uploader = widgets.FileUpload(accept=\".json\", multiple=False)\n",
        "dev_uploader = widgets.FileUpload(accept=\".json\", multiple=False)\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_save(_):\n",
        "    with out:\n",
        "        out.clear_output()\n",
        "        if not train_uploader.value or not dev_uploader.value:\n",
        "            print(\"Please upload both train.json and dev.json\")\n",
        "            return\n",
        "        t_name, t_meta = next(iter(train_uploader.value.items()))\n",
        "        d_name, d_meta = next(iter(dev_uploader.value.items()))\n",
        "        TRAIN_JSON_PATH.write_bytes(t_meta[\"content\"])\n",
        "        DEV_JSON_PATH.write_bytes(d_meta[\"content\"])\n",
        "        print(f\"Saved train.json → {TRAIN_JSON_PATH}\")\n",
        "        print(f\"Saved dev.json   → {DEV_JSON_PATH}\")\n",
        "\n",
        "btn = widgets.Button(description=\"Save files\", button_style=\"success\")\n",
        "btn.on_click(on_save)\n",
        "\n",
        "display(widgets.HTML(\"<b>Upload train.json</b>\"), train_uploader)\n",
        "display(widgets.HTML(\"<b>Upload dev.json</b>\"), dev_uploader)\n",
        "display(btn, out)"
      ],
      "metadata": {
        "id": "9q3yndo61Kc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages:\n"
      ],
      "metadata": {
        "id": "0nnaBss_2AWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==3.7.4 spacy-transformers==1.3.4 transformers==4.36.2"
      ],
      "metadata": {
        "id": "976m9wnD1_Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "id": "3AhaOB1h2MmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Convert data from json to spacy format"
      ],
      "metadata": {
        "id": "roHP3hWK1JeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, json, srsly, pathlib\n",
        "from spacy.tokens import DocBin\n",
        "import spacy\n",
        "\n",
        "TARGET_LABELS = {\"Skill\", \"Work_Experience\", \"Education\", \"Language\"}\n",
        "\n",
        "def iter_tasks(obj):\n",
        "    \"\"\"Yield Label Studio task objects from either JSON array or JSONL file.\"\"\"\n",
        "    p = pathlib.Path(obj)\n",
        "    txt = p.read_text(encoding=\"utf-8-sig\")\n",
        "    if txt.lstrip().startswith(\"[\"):\n",
        "        for rec in json.loads(txt):\n",
        "            yield rec\n",
        "    else:\n",
        "        for line in txt.splitlines():\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            yield json.loads(line)\n",
        "\n",
        "def extract_spans(task, use_predictions=False):\n",
        "    \"\"\"\n",
        "    Return list of (start, end, label) from a LS task.\n",
        "    If your export stores gold labels under 'annotations', keep use_predictions=False.\n",
        "    If yours uses 'predictions', set use_predictions=True.\n",
        "    \"\"\"\n",
        "    key = \"predictions\" if use_predictions else \"annotations\"\n",
        "    spans = []\n",
        "    for ann in task.get(key, []):\n",
        "        for r in ann.get(\"result\", []):\n",
        "            if r.get(\"type\") != \"labels\":\n",
        "                continue\n",
        "            labels = r[\"value\"].get(\"labels\", [])\n",
        "            if not labels:\n",
        "                continue\n",
        "            label = labels[0]\n",
        "            if label not in TARGET_LABELS:\n",
        "                continue\n",
        "            start = r[\"value\"][\"start\"]\n",
        "            end = r[\"value\"][\"end\"]\n",
        "            spans.append((start, end, label))\n",
        "    return spans\n",
        "\n",
        "def make_docbin(nlp, tasks, use_predictions=False):\n",
        "    db = DocBin(store_user_data=False)\n",
        "    bad, good = 0, 0\n",
        "    for t in tasks:\n",
        "        text = (t.get(\"data\") or {}).get(\"text\") or t.get(\"text\") or \"\"\n",
        "        if not text:\n",
        "            continue\n",
        "        doc = nlp.make_doc(text)\n",
        "        ents = []\n",
        "        spans = extract_spans(t, use_predictions=use_predictions)\n",
        "        # filter overlapping/invalid spans\n",
        "        char_taken = [False] * (len(text) + 1)\n",
        "        for start, end, label in sorted(spans, key=lambda x: (x[0], x[1])):\n",
        "            if start >= end or end > len(text):\n",
        "                continue\n",
        "            if any(char_taken[start:end]):\n",
        "                continue\n",
        "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
        "            if span is None:\n",
        "                continue\n",
        "            for i in range(start, end):\n",
        "                char_taken[i] = True\n",
        "            ents.append(span)\n",
        "        doc.ents = ents\n",
        "        if ents:\n",
        "            good += 1\n",
        "        else:\n",
        "            bad += 1\n",
        "        db.add(doc)\n",
        "    print(f\"Built {good} docs with entities; {bad} had none.\")\n",
        "    return db\n",
        "\n",
        "\n",
        "lang, train_in, dev_in, out_dir = \"en\", \"./annotated/train.json\", \"./annotated/dev.json\", \".\"\n",
        "use_pred = False  # Default value\n",
        "\n",
        "# Create tokenizer base for Doc creation\n",
        "nlp = spacy.blank(\"en\") if lang == \"en\" else spacy.blank(\"de\")\n",
        "\n",
        "# Load the tasks\n",
        "train_tasks = list(iter_tasks(train_in))\n",
        "dev_tasks = list(iter_tasks(dev_in))\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "out = pathlib.Path(out_dir)\n",
        "out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create DocBin objects\n",
        "train_db = make_docbin(nlp, train_tasks, use_predictions=use_pred)\n",
        "dev_db = make_docbin(nlp, dev_tasks, use_predictions=use_pred)\n",
        "\n",
        "# Save to disk\n",
        "train_db.to_disk(out / \"train.spacy\")\n",
        "dev_db.to_disk(out / \"dev.spacy\")\n",
        "\n",
        "print(f\"Wrote {out/'train.spacy'} and {out/'dev.spacy'}\")"
      ],
      "metadata": {
        "id": "dZxheQBG1TfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Set up model config:"
      ],
      "metadata": {
        "id": "QR25_06g1YR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = \"\"\"[paths]\n",
        "train = null\n",
        "dev = null\n",
        "vectors = null\n",
        "init_tok2vec = null\n",
        "\n",
        "[system]\n",
        "gpu_allocator = \"pytorch\"\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"transformer\",\"ner\"]\n",
        "batch_size = 128\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.ner]\n",
        "factory = \"ner\"\n",
        "incorrect_spans_key = null\n",
        "moves = null\n",
        "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
        "update_with_oracle_cut_size = 100\n",
        "\n",
        "[components.ner.model]\n",
        "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
        "state_type = \"ner\"\n",
        "extra_state_tokens = false\n",
        "hidden_width = 64\n",
        "maxout_pieces = 2\n",
        "use_upper = false\n",
        "nO = null\n",
        "\n",
        "[components.ner.model.tok2vec]\n",
        "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
        "grad_factor = 1.0\n",
        "pooling = {\"@layers\":\"reduce_mean.v1\"}\n",
        "upstream = \"*\"\n",
        "\n",
        "[components.transformer]\n",
        "factory = \"transformer\"\n",
        "max_batch_items = 4096\n",
        "set_extra_annotations = {\"@annotation_setters\":\"spacy-transformers.null_annotation_setter.v1\"}\n",
        "\n",
        "[components.transformer.model]\n",
        "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
        "name = \"distilbert-base-uncased\"\n",
        "mixed_precision = false\n",
        "\n",
        "[components.transformer.model.get_spans]\n",
        "@span_getters = \"spacy-transformers.strided_spans.v1\"\n",
        "window = 128\n",
        "stride = 96\n",
        "\n",
        "[components.transformer.model.grad_scaler_config]\n",
        "\n",
        "[components.transformer.model.tokenizer_config]\n",
        "use_fast = true\n",
        "\n",
        "[components.transformer.model.transformer_config]\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "gold_preproc = false\n",
        "limit = 0\n",
        "augmenter = null\n",
        "\n",
        "[training]\n",
        "accumulate_gradient = 3\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "dropout = 0.1\n",
        "patience = 1600\n",
        "max_epochs = 0\n",
        "max_steps = 20000\n",
        "eval_frequency = 200\n",
        "frozen_components = []\n",
        "annotating_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_padded.v1\"\n",
        "discard_oversize = true\n",
        "size = 2000\n",
        "buffer = 256\n",
        "get_length = null\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = true\n",
        "L2 = 0.01\n",
        "grad_clip = 1.0\n",
        "use_averages = false\n",
        "eps = 0.00000001\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"warmup_linear.v1\"\n",
        "warmup_steps = 250\n",
        "total_steps = 20000\n",
        "initial_rate = 0.00005\n",
        "\n",
        "[training.score_weights]\n",
        "ents_f = 1.0\n",
        "ents_p = 0.0\n",
        "ents_r = 0.0\n",
        "ents_per_type = null\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "vectors = ${paths.vectors}\n",
        "init_tok2vec = ${paths.init_tok2vec}\n",
        "vocab_data = null\n",
        "lookups = null\n",
        "before_init = null\n",
        "after_init = null\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.cfg\", \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(\"Config file generated!\")"
      ],
      "metadata": {
        "id": "xIg_Axwn1pbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Train the model:"
      ],
      "metadata": {
        "id": "oYIAacPk1pqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy --gpu-id 0"
      ],
      "metadata": {
        "id": "D4Kbi8Y81s9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Save the model to the Google drive"
      ],
      "metadata": {
        "id": "QM8o4SIw4YrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/output/model-best /content/drive/MyDrive/resume-parser-models/core-web-lg/"
      ],
      "metadata": {
        "id": "QHs_QB0J4wQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}