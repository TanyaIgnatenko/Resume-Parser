# ğŸ“„ Resume Parser (AI-powered)

An end-to-end **Resume Parser** that extracts structured information (Skills, Work Experience, Education, Languages) from resumes in **PDF/DOCX/TXT** formats.  
Built with **spaCy-transformers (RoBERTa-base fine-tuned)**, **FastAPI backend**, and **React frontend** - achieving **97.50% F1-score** on resume entity recognition.

---

## ğŸš€ Project Highlights

![Python](https://img.shields.io/badge/Python-3.10-blue)
![spaCy](https://img.shields.io/badge/NLP-spaCy_transformers-green)
![FastAPI](https://img.shields.io/badge/API-FastAPI-teal)
![React](https://img.shields.io/badge/Frontend-React-blue)
![Dataset](https://img.shields.io/badge/Dataset-60%20resumes-orange)
![NER F1](https://img.shields.io/badge/F1%20Score-97.50%25-brightgreen)
![Model](https://img.shields.io/badge/Model-RoBERTa--base-red)

- **Dataset:** 60 resumes (42 train, 8 dev, 10 test)  
- **Model:** **spaCy transformer** (RoBERTa-base fine-tuned)  
- **Best Test F1-score:** **97.50%** (overall)  
- **Entities extracted:** Skills, Work Experience, Education, Languages  

---

## ğŸ–¼ï¸ Screenshots

**Upload Page**  
![Upload Resume Page](./screenshots/upload-2.png)

**Parsed Resume Results**  
![Parsed Resume Results](./screenshots/results-3.png)

---

## âœ¨ Features
- ğŸ“‚ Upload resumes (PDF, DOCX, TXT).  
- ğŸ¤– AI-powered extraction of:
  - ğŸ¯ **Skills**
  - ğŸ’¼ **Work Experience**
  - ğŸ“ **Education**
  - ğŸŒ **Languages**  
- ğŸ“Š Instant structured output for downstream use.  
- ğŸ’» Web interface + REST API.  

---

## ğŸ› ï¸ Tech Stack & Tools

- **Languages:** Python, TypeScript  
- **NLP Framework:** **spaCy transformers** (RoBERTa-base)
- **Backend:** FastAPI, Uvicorn  
- **Frontend:** React / Next.js  
- **Parsing Tools:** pdfplumber, docx2txt  
- **Data Annotation:** Label Studio  
- **Pretrained Model:** RoBERTa-base
- **DevOps & Tools:** Git, Docker, AWS, Vercel
- **Methodologies:** Data preprocessing, tokenization, regex bootstrapping, model evaluation with F1/Precision/Recall  

---

## ğŸ§  Model Architecture

The resume parser now uses a **fine-tuned transformer model** for superior entity recognition:

### Transformer Backbone
- **Base Model:** **RoBERTa-base** (125M parameters)
- **Framework:** spaCy-transformers
- **Training:** Fine-tuned on resume domain

### Training Configuration
- **Learning Rate:** 5e-5 with linear warmup
- **Batch Size:** 128 with dynamic padding
- **Max Steps:** 20,000 with early stopping (patience: 1,600 steps)
- **Optimizer:** AdamW with weight decay

### Advantages over Previous Model
- âœ… **Better contextual understanding** of resume text
- âœ… **Improved generalization** to unseen resume formats
- âœ… **Higher accuracy** on complex entity boundaries
- âœ… **State-of-the-art** transformer architecture
---
## ğŸ§  Model Training

The **spaCy transformer model** was fine-tuned on manually annotated resumes using transfer learning:

- **Base Model:** RoBERTa-base (pretrained on general text)
- **Fine-tuning Dataset:** 60 annotated resumes  
- **Split:**  
  - 42 resumes â†’ Training  
  - 8 resumes â†’ Development (validation)  
  - 10 resumes â†’ Test (final evaluation)  

### Training Pipeline:
- Text extraction & normalization from PDF/DOCX
- Transformer tokenization with RoBERTa tokenizer
- Fine-tuning with spaCy's transformer NER architecture
- Early stopping to prevent overfitting
- Comprehensive evaluation on held-out test set

---

## ğŸ“Š Evaluation Results

On the held-out **test set (10 resumes)**:

| Metric | Score |
|--------|-------|
| **Precision (NER P)** | 91.12% |
| **Recall (NER R)**    | 91.55% |
| **F1-score (NER F)**  | 91.33% |

### Per-entity performance:

| Entity     | Precision | Recall | F1    |
|------------|-----------|--------|-------|
| ğŸ“ Education | 86.21     | 75.76  | 80.65 |
| ğŸ’¼ Work Exp. | 69.23     | 81.82  | 75.00 |
| ğŸ¯ Skill     | 94.96     | 95.65  | 95.31 |
| ğŸŒ Language  | 100.00    | 100.00 | 100.00 |


âš¡ **Why not accuracy?**  
For NER tasks, **accuracy is misleading** because the majority of tokens are *not entities*. Instead, the **F1-score (harmonic mean of precision & recall)** is the gold-standard metric in NLP.  

---

## ğŸ”§ Currently Improving Model Accuracy

I'm currently improving the accuracy of the Resume Parser using these methodologies:

### ğŸ“Š Data-Centric
- Expanding dataset with more annotated resumes (beyond current 50).  
- Annotating more **Work Experience** and **Education** entities to address weaker categories.  
- Reviewing and enforcing **annotation consistency** (e.g., clear rules for job titles, companies, skills).  

### ğŸ¤– Model-Centric
- Experimenting with larger **spaCy models** (`en_core_web_md`, `en_core_web_lg`).  
- Fine-tuning **transformer-based models** (BERT, RoBERTa) using `spacy-transformers`.  
- Hyperparameter tuning: adjusting **learning rate, batch size, training iterations**.

---

## âš¡ Installation & Usage

### 1. Clone repo
```bash
git clone https://github.com/yourusername/resume-parser.git
cd resume-parser
```

### 2. Setup backend
```bash
cd backend
pip install -r requirements.txt
uvicorn main:app --reload
```

### 3. Setup frontend
```bash
cd frontend
npm install
npm run dev
```

### 4. Test API
```bash 
curl -X POST "http://localhost:8000/parse" \
     -F "file=@sample_resume.pdf"
```

## ğŸ“Œ License

MIT License. Free to use & modify.
